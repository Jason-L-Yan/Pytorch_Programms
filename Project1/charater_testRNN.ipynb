{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字符排序\n",
    "- 目标：\n",
    "\n",
    "![](https://cdn.jsdelivr.net/gh/Jason-L-Yan/imgBed/img/image-20200706212357924.png)\n",
    "- 原理\n",
    "\n",
    "![](https://cdn.jsdelivr.net/gh/Jason-L-Yan/imgBed/img/image-20200706212750115.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "把字符进行 one-hot 编码，输入与标签维度定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 4\n",
    "batch_size = 1  # 一个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx2char = ['e', 'h', 'l', 'o']\n",
    "x_data = [1, 0, 2, 2, 3]  # The input sequence is 'hello'\n",
    "y_data = [3, 1, 2, 3, 2]  # The output sequence is 'ohlol'\n",
    "\n",
    "# one-hot encoding\n",
    "# pattern 1. \n",
    "one_hot_lookup = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "# pattern 2. \n",
    "# words = torch.tensor([0, 1, 2, 3], dtype=torch.long)\n",
    "# one_hot_encoding = one_hot(words)\n",
    "# print(one_hot_encoding)\n",
    "\n",
    "# Convert indices into one-hot vector\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]  # [seqLen, input_size]\n",
    "# Reshape the inputs to (seqLen, batch_size, input_size) \n",
    "inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size)\n",
    "labels = torch.LongTensor(y_data).view(-1, 1)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 RNNCell 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.RNNcell = torch.nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.RNNcell(input, hidden)\n",
    "        return hidden \n",
    "        \n",
    "    def init_hidden(self):  # provide initial hidden(h0)\n",
    "        return torch.zeros(self.batch_size, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model(input_size, hidden_size, batch_size)  # 4, 4, 1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "======================== 0 ========================\nPredicted string: ollll, Epocn [1 / 15] loss=5.4837\n======================== 1 ========================\nPredicted string: ollll, Epocn [2 / 15] loss=4.9311\n======================== 2 ========================\nPredicted string: ollll, Epocn [3 / 15] loss=4.4451\n======================== 3 ========================\nPredicted string: ohlll, Epocn [4 / 15] loss=3.9837\n======================== 4 ========================\nPredicted string: ohlol, Epocn [5 / 15] loss=3.6430\n======================== 5 ========================\nPredicted string: ohlol, Epocn [6 / 15] loss=3.4190\n======================== 6 ========================\nPredicted string: ohlol, Epocn [7 / 15] loss=3.1067\n======================== 7 ========================\nPredicted string: ohlol, Epocn [8 / 15] loss=2.8451\n======================== 8 ========================\nPredicted string: ohlol, Epocn [9 / 15] loss=2.6792\n======================== 9 ========================\nPredicted string: ohlol, Epocn [10 / 15] loss=2.5638\n======================== 10 ========================\nPredicted string: ohlol, Epocn [11 / 15] loss=2.4677\n======================== 11 ========================\nPredicted string: ohlol, Epocn [12 / 15] loss=2.3780\n======================== 12 ========================\nPredicted string: ohlol, Epocn [13 / 15] loss=2.2892\n======================== 13 ========================\nPredicted string: ohlol, Epocn [14 / 15] loss=2.2024\n======================== 14 ========================\nPredicted string: ohlol, Epocn [15 / 15] loss=2.1249\n"
    }
   ],
   "source": [
    "i = 0\n",
    "for epoch in range(15):\n",
    "    print('=' * 24, i, '=' * 24)\n",
    "    i += 1\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    hidden = net.init_hidden()\n",
    "    print('Predicted string: ', end='')\n",
    "    # shape of inputs:(seqLen, batch_size, input_size)\n",
    "    # shape of input :(batch_size, input_size)\n",
    "    # shape of labels:(seqLen, 1)\n",
    "    # shape of label :(1)\n",
    "    for input, label in zip(inputs, labels): \n",
    "        # 通过不断地 for 循环，使这次的 hidden 结果，输入到下次。 \n",
    "        hidden = net(input, hidden)\n",
    "        # 不要用loss.item()，因为一个循环只求出了一个seq的损失，要把所有seq的损失和加起来构造计算图，如上方最后一张图所示，所有seq的损失的和，才是最终的损失。\n",
    "        loss += criterion(hidden, label)  \n",
    "        _, idx = hidden.max(dim=1)  # 返回每一行最大概率的下标\n",
    "        print(idx2char[idx.item()], end='')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(', Epocn [%d / 15] loss=%.4f' % (epoch + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size2 = 4\n",
    "hidden_size2 = 4\n",
    "num_layers2 = 1\n",
    "batch_size2 = 1  # 一个样本\n",
    "seq_len2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char2 = ['e', 'h', 'l', 'o']\n",
    "x_data2 = [1, 0, 2, 2, 3]  # The input sequence is 'hello'\n",
    "y_data2 = [3, 1, 2, 3, 2]  # The output sequence is 'ohlol'\n",
    "one_hot_lookup2 = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "# Convert indices into one-hot vector\n",
    "x_one_hot2 = [one_hot_lookup2[x2] for x2 in x_data2]  # [seqLen, input_size]\n",
    "# Reshape the inputs to (seqLen, batch_size, input_size) \n",
    "inputs2 = torch.Tensor(x_one_hot2).view(seq_len2, batch_size2, input_size2)\n",
    "labels2 = torch.LongTensor(y_data2)  # 不同于上一个程序，(seqLen*batch_size, 1), 呼应Model2中forward的返回值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 RNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(torch.nn.Module):\n",
    "    def __init__(self, input_size2, hidden_size2, batch_size2, num_layers2):\n",
    "        super(Model2, self).__init__()\n",
    "        self.num_layers2 = num_layers2\n",
    "        self.batch_size2 = batch_size2\n",
    "        self.input_size2 = input_size2\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.RNN = torch.nn.RNN(input_size=self.input_size2, \n",
    "                                hidden_size=self.hidden_size2,                                                                                  num_layers=self.num_layers2)\n",
    "\n",
    "    def forward(self, input2):\n",
    "        # shape of hidden:(num_layers, batch_size, hidden_size)\n",
    "        hidden2 = torch.zeros(self.num_layers2, self.batch_size2, self.hidden_size2)\n",
    "        out, _ = self.RNN(input2, hidden2)\n",
    "        return out.view(-1, self.hidden_size2)  # Reshape out to (seqLen*batch_size, hidden_size), 变成了一个矩阵，方便计算交叉熵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Model2(input_size2, hidden_size2, batch_size2, num_layers2)\n",
    "criterion2 = torch.nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.Adam(net2.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predicted:  lllle, Epocn [1 / 15] loss=1.643\nPredicted:  lllle, Epocn [2 / 15] loss=1.489\nPredicted:  lllll, Epocn [3 / 15] loss=1.371\nPredicted:  lllll, Epocn [4 / 15] loss=1.262\nPredicted:  hllll, Epocn [5 / 15] loss=1.154\nPredicted:  hhlll, Epocn [6 / 15] loss=1.059\nPredicted:  hhlll, Epocn [7 / 15] loss=0.983\nPredicted:  hhlll, Epocn [8 / 15] loss=0.925\nPredicted:  hhlll, Epocn [9 / 15] loss=0.879\nPredicted:  hhlll, Epocn [10 / 15] loss=0.839\nPredicted:  ohlll, Epocn [11 / 15] loss=0.805\nPredicted:  ohlll, Epocn [12 / 15] loss=0.777\nPredicted:  ohlll, Epocn [13 / 15] loss=0.752\nPredicted:  ohlll, Epocn [14 / 15] loss=0.729\nPredicted:  ohlll, Epocn [15 / 15] loss=0.709\n"
    }
   ],
   "source": [
    "loss2 = 0\n",
    "for epoch in range(15):\n",
    "    optimizer2.zero_grad()\n",
    "    outputs = net2(inputs2)\n",
    "    loss2 = criterion2(outputs, labels2)\n",
    "    loss2.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    _, idx2 = outputs.max(dim=1)\n",
    "    idx2 = idx2.data.numpy()\n",
    "    print('Predicted: ', ''.join([idx2char2[x] for x in idx2]), end='')\n",
    "    print(', Epocn [%d / 15] loss=%.3f' % (epoch + 1, loss2.item()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594191035317",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}