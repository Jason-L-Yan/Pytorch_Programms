{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
    }
   ],
   "source": [
    "sentence = \"the quick brown fox jumped over the lazy dog\"\n",
    "words = sentence.split(' ')  # 分词\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'n', 'b', 'r', 'u', 'o'} {'g', 'l', 'e', 'o'}\n"
    }
   ],
   "source": [
    "x = set('runoob') \n",
    "y = set('google')\n",
    "print(x, y)  # 重复的‘o’被删除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归主题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['jumped', 'the', 'quick', 'brown', 'over', 'dog', 'lazy', 'fox']\n"
    }
   ],
   "source": [
    "vocabl = list(set(words))  # 去除了重复的 ‘the’\n",
    "print(vocabl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9\n8\n"
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(vocabl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding Example\n",
    "需要纬度高，编码稀疏（信息量少）等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'jumped': 0, 'the': 1, 'quick': 2, 'brown': 3, 'over': 4, 'dog': 5, 'lazy': 6, 'fox': 7}\n"
    }
   ],
   "source": [
    "# convert words to indexes\n",
    "word_to_ix1 = {word : i for i, word in enumerate(vocabl)}  # convert to dict\n",
    "print(word_to_ix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n['jumped', 'the', 'quick', 'brown', 'over', 'dog', 'lazy', 'fox']\ntensor([[1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1]])\n"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "words = torch.tensor([word_to_ix1[w] for w in vocabl], dtype=torch.long)\n",
    "print(words)\n",
    "one_hot_encoding = one_hot(words)  # one-hot编码\n",
    "print(vocabl)\n",
    "print(one_hot_encoding)  # 信息量太少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context is the number of words we are using as a context for the next word we want to predict.\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# Embedding dimension is the size of embedding vector\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# Size of the hidden Layer\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"Tomorrow and tomorrow and tomorrow,\n",
    "Creeps in this petty pace from day to day\n",
    "To the last syllable of recorded time,\n",
    "And all our yesterdays have lighted fools\n",
    "The way to dusty death. Out, out, brief candle!\n",
    "Life's but a walking shadow, a poor player\n",
    "That struts and frets his hour upon the stage\n",
    "And then is heard no more: it is a tale\n",
    "Told by an idiot, full of sound and fury,\n",
    "Signifying nothing.\"\"\".lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(['tomorrow', 'and'], 'tomorrow'), (['and', 'tomorrow'], 'and'), (['tomorrow', 'and'], 'tomorrow,'), (['and', 'tomorrow,'], 'creeps'), (['tomorrow,', 'creeps'], 'in'), (['creeps', 'in'], 'this')]\n====================================================================================================\n['last', 'petty', 'by', 'in', 'from', 'out,', 'nothing.', 'recorded', 'yesterdays', 'fury,', 'a', 'but', 'no', 'more:', 'sound', 'the', 'pace', 'death.', 'that', 'then', 'dusty', 'brief', 'tale', 'this', 'time,', 'frets', 'signifying', 'his', 'walking', 'hour', 'poor', 'upon', 'is', 'to', 'syllable', 'candle!', 'have', 'heard', 'creeps', 'day', 'lighted', 'it', 'idiot,', \"life's\", 'struts', 'and', 'way', 'fools', 'our', 'stage', 'all', 'shadow,', 'tomorrow,', 'an', 'told', 'player', 'of', 'full', 'tomorrow']\n====================================================================================================\n75\n59\n====================================================================================================\n{'last': 0, 'petty': 1, 'by': 2, 'in': 3, 'from': 4, 'out,': 5, 'nothing.': 6, 'recorded': 7, 'yesterdays': 8, 'fury,': 9, 'a': 10, 'but': 11, 'no': 12, 'more:': 13, 'sound': 14, 'the': 15, 'pace': 16, 'death.': 17, 'that': 18, 'then': 19, 'dusty': 20, 'brief': 21, 'tale': 22, 'this': 23, 'time,': 24, 'frets': 25, 'signifying': 26, 'his': 27, 'walking': 28, 'hour': 29, 'poor': 30, 'upon': 31, 'is': 32, 'to': 33, 'syllable': 34, 'candle!': 35, 'have': 36, 'heard': 37, 'creeps': 38, 'day': 39, 'lighted': 40, 'it': 41, 'idiot,': 42, \"life's\": 43, 'struts': 44, 'and': 45, 'way': 46, 'fools': 47, 'our': 48, 'stage': 49, 'all': 50, 'shadow,': 51, 'tomorrow,': 52, 'an': 53, 'told': 54, 'player': 55, 'of': 56, 'full': 57, 'tomorrow': 58}\n"
    }
   ],
   "source": [
    "# Build a list of tuples. Each tuple is ([word_i-2, word_i-1], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "             for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[: 6])\n",
    "vocab2 = list(set(test_sentence))\n",
    "print('=='*50)\n",
    "print(vocab2)\n",
    "print('=='*50)\n",
    "print(len(test_sentence))\n",
    "print(len(vocab2))\n",
    "word_to_ix2 = {word : i for i, word in enumerate(vocab2)}\n",
    "print('=='*50)\n",
    "print(word_to_ix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, HIDDEN_DIM)\n",
    "        self.linear2 = nn.Linear(HIDDEN_DIM, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()  # negative log likehood\n",
    "model = NGramLanguageModeler(len(vocab2), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 73/73 [00:02<00:00, 34.55it/s, loss=4.5]\n100%|██████████| 73/73 [00:02<00:00, 30.08it/s, loss=4.46]\n100%|██████████| 73/73 [00:03<00:00, 22.43it/s, loss=4.42]\n100%|██████████| 73/73 [00:01<00:00, 36.54it/s, loss=4.38]\n100%|██████████| 73/73 [00:02<00:00, 29.75it/s, loss=4.34]\n100%|██████████| 73/73 [00:01<00:00, 53.40it/s, loss=4.3]\n100%|██████████| 73/73 [00:01<00:00, 37.98it/s, loss=4.26]\n100%|██████████| 73/73 [00:02<00:00, 35.78it/s, loss=4.22]\n100%|██████████| 73/73 [00:01<00:00, 46.44it/s, loss=4.18]\n100%|██████████| 73/73 [00:02<00:00, 32.79it/s, loss=4.15]\n100%|██████████| 73/73 [00:02<00:00, 33.60it/s, loss=4.11]\n100%|██████████| 73/73 [00:02<00:00, 32.43it/s, loss=4.07]\n100%|██████████| 73/73 [00:02<00:00, 34.24it/s, loss=4.03]\n100%|██████████| 73/73 [00:01<00:00, 39.31it/s, loss=4]\n100%|██████████| 73/73 [00:01<00:00, 38.18it/s, loss=3.96]\n100%|██████████| 73/73 [00:02<00:00, 30.38it/s, loss=3.92]\n100%|██████████| 73/73 [00:02<00:00, 31.14it/s, loss=3.88]\n100%|██████████| 73/73 [00:01<00:00, 44.24it/s, loss=3.85]\n100%|██████████| 73/73 [00:02<00:00, 25.21it/s, loss=3.81]\n100%|██████████| 73/73 [00:02<00:00, 28.01it/s, loss=3.77]\n100%|██████████| 73/73 [00:02<00:00, 35.40it/s, loss=3.74]\n100%|██████████| 73/73 [00:01<00:00, 46.89it/s, loss=3.7]\n100%|██████████| 73/73 [00:02<00:00, 32.33it/s, loss=3.66]\n100%|██████████| 73/73 [00:03<00:00, 18.38it/s, loss=3.62]\n100%|██████████| 73/73 [00:01<00:00, 41.36it/s, loss=3.59]\n100%|██████████| 73/73 [00:02<00:00, 36.26it/s, loss=3.55]\n100%|██████████| 73/73 [00:01<00:00, 61.19it/s, loss=3.51]\n100%|██████████| 73/73 [00:02<00:00, 31.74it/s, loss=3.48]\n100%|██████████| 73/73 [00:01<00:00, 44.46it/s, loss=3.44]\n100%|██████████| 73/73 [00:02<00:00, 30.99it/s, loss=3.4]\n100%|██████████| 73/73 [00:02<00:00, 25.12it/s, loss=3.37]\n100%|██████████| 73/73 [00:01<00:00, 61.03it/s, loss=3.33]\n100%|██████████| 73/73 [00:02<00:00, 26.92it/s, loss=3.29]\n100%|██████████| 73/73 [00:02<00:00, 34.68it/s, loss=3.26]\n100%|██████████| 73/73 [00:01<00:00, 44.73it/s, loss=3.22]\n100%|██████████| 73/73 [00:00<00:00, 114.59it/s, loss=3.18]\n100%|██████████| 73/73 [00:01<00:00, 40.44it/s, loss=3.15]\n100%|██████████| 73/73 [00:02<00:00, 32.37it/s, loss=3.11]\n100%|██████████| 73/73 [00:02<00:00, 35.80it/s, loss=3.07]\n100%|██████████| 73/73 [00:01<00:00, 38.71it/s, loss=3.04]\n100%|██████████| 73/73 [00:01<00:00, 41.67it/s, loss=3]\n100%|██████████| 73/73 [00:01<00:00, 40.62it/s, loss=2.96]\n100%|██████████| 73/73 [00:02<00:00, 28.98it/s, loss=2.93]\n100%|██████████| 73/73 [00:02<00:00, 31.34it/s, loss=2.89]\n100%|██████████| 73/73 [00:01<00:00, 40.62it/s, loss=2.85]\n100%|██████████| 73/73 [00:03<00:00, 22.31it/s, loss=2.82]\n100%|██████████| 73/73 [00:02<00:00, 25.03it/s, loss=2.78]\n100%|██████████| 73/73 [00:01<00:00, 41.67it/s, loss=2.75]\n100%|██████████| 73/73 [00:03<00:00, 19.40it/s, loss=2.71]\n100%|██████████| 73/73 [00:03<00:00, 20.73it/s, loss=2.67]\n"
    }
   ],
   "source": [
    "from tqdm import tqdm  # 加上进度条\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    iterator = tqdm(trigrams)\n",
    "    for context, target in iterator:\n",
    "        # Step 1. Prepare the inputs to be passed to the model(i.e, turn the words into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a new instance,you need to zero out gradients from the old instance.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass,getting log probabilities over next words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again,Torch wants the target word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix2[target]], dtype=torch.long))\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-elements Tensor calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        iterator.set_postfix(loss=float(loss))  # 在进度条后面加上损失值 loss\n",
    "    losses.append(total_loss)\n",
    "    # add progress bar with epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the structure of our model here\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-4.4783, -4.4191, -3.7023, -5.1273, -4.2793, -4.2370, -4.6083, -3.9433,\n         -4.2768, -4.4262, -2.8790, -3.8211, -4.4354, -3.7915, -4.1032, -3.1344,\n         -4.4880, -4.8107, -4.3386, -5.1267, -4.4028, -4.3076, -4.0304, -2.6152,\n         -4.3099, -4.5722, -4.1308, -4.7391, -4.6606, -3.9616, -4.4295, -4.6521,\n         -4.1112, -3.6293, -4.5858, -4.3190, -4.6619, -5.0203, -4.2950, -4.3610,\n         -4.1950, -4.5477, -4.9065, -4.2715, -4.2793, -2.9366, -4.3579, -4.1131,\n         -4.5759, -4.4559, -4.4987, -4.1404, -4.7564, -4.8629, -4.3767, -3.7872,\n         -2.7306, -4.6732, -4.6658]])\ntorch.FloatTensor\nthis\n"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "with torch.no_grad():\n",
    "    context = ['creeps', 'in']\n",
    "    context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
    "    pred = model(context_idxs)\n",
    "    print(pred)\n",
    "    print(pred.type())\n",
    "    index_of_prediction = numpy.argmax(pred)\n",
    "    print(vocab2[index_of_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594105731385",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}