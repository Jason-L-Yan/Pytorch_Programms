{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence = \"the quick brown fox jumped over the lazy dog\"\n",
    "words = sentence.split(' ')  # 分词\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = set('runoob') \n",
    "y = set('google')\n",
    "print(x, y)  # 重复的‘o’被删除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归主题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocabl = list(set(words))  # 去除了重复的 ‘the’\n",
    "print(vocabl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(words))\n",
    "print(len(vocabl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding Example\n",
    "需要纬度高，编码稀疏（信息量少）等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert words to indexes\n",
    "word_to_ix1 = {word : i for i, word in enumerate(vocabl)}  # convert to dict\n",
    "print(word_to_ix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "words = torch.tensor([word_to_ix1[w] for w in vocabl], dtype=torch.long)\n",
    "print(words)\n",
    "one_hot_encoding = one_hot(words)  # one-hot编码\n",
    "print(vocabl)\n",
    "print(one_hot_encoding)  # 信息量太少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context is the number of words we are using as a context for the next word we want to predict.\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# Embedding dimension is the size of embedding vector\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# Size of the hidden Layer\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"Tomorrow and tomorrow and tomorrow,\n",
    "Creeps in this petty pace from day to day\n",
    "To the last syllable of recorded time,\n",
    "And all our yesterdays have lighted fools\n",
    "The way to dusty death. Out, out, brief candle!\n",
    "Life's but a walking shadow, a poor player\n",
    "That struts and frets his hour upon the stage\n",
    "And then is heard no more: it is a tale\n",
    "Told by an idiot, full of sound and fury,\n",
    "Signifying nothing.\"\"\".lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a list of tuples. Each tuple is ([word_i-2, word_i-1], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "             for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[: 6])\n",
    "vocab2 = list(set(test_sentence))\n",
    "print('=='*50)\n",
    "print(vocab2)\n",
    "print('=='*50)\n",
    "print(len(test_sentence))\n",
    "print(len(vocab2))\n",
    "word_to_ix2 = {word : i for i, word in enumerate(vocab2)}\n",
    "print('=='*50)\n",
    "print(word_to_ix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):  # 59, 10, 2\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "         # vocab_size：嵌入层字典的大小（单词本里单词个数），embedding_dim: 每个产出向量的大小\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) \n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, HIDDEN_DIM)\n",
    "        self.linear2 = nn.Linear(HIDDEN_DIM, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        \"\"\"因为是根据前两个单词，去预测第三个。input是输入的两个行索引，每行10列，每一行代表一个单词，其中Embedding是初始化好的矩阵，行数为单词本里单词个数，列数为embedding_dim。输入索引，取出两行 10 维向量，当做网络的输入数据去训练、去反向传播\"\"\"\n",
    "\n",
    "        embeds = self.embedding(inputs)  # embeds:  torch.Size([2, 10])\n",
    "        embeds = embeds.view((1, -1))  # embeds:  torch.Size([1, 20])\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()  # negative log likehood\n",
    "model = NGramLanguageModeler(len(vocab2), EMBEDDING_DIM, CONTEXT_SIZE)  # 59, 10, 2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # 加上进度条\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    iterator = tqdm(trigrams)\n",
    "    for context, target in iterator:\n",
    "        # Step 1. Prepare the inputs to be passed to the model(i.e, turn the words into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a new instance,you need to zero out gradients from the old instance.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass,getting log probabilities over next words\n",
    "        log_probs = model(context_idxs)  # torch.Size([1, 59])\n",
    "        \n",
    "        # Step 4. Compute your loss function. (Again,Torch wants the target word wrapped in a tensor)\n",
    "        # log_probs是torch.Size([1, 59]), 而torch.tensor([word_to_ix2[target]], dtype=torch.long) 是一个具体的LongTensor型数字，它会被自动转换为one-hot进行训练\n",
    "        \n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix2[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-elements Tensor calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        iterator.set_postfix(loss=float(loss))  # 在进度条后面加上损失值 loss\n",
    "    losses.append(total_loss)\n",
    "    # add progress bar with epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the structure of our model here\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "with torch.no_grad():\n",
    "    context3 = ['tomorrow', 'and']\n",
    "    context_idxs3 = torch.tensor([word_to_ix2[w] for w in context3], dtype=torch.long)\n",
    "    pred = model(context_idxs3)  # torch.Size([1, 59])\n",
    "    index_of_prediction = numpy.argmax(pred)\n",
    "    print(vocab2[index_of_prediction])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596075519576",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}